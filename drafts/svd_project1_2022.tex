\documentclass[11.5pt]{article}
\usepackage{tikz-cd}
\usepackage{tikz}
\usetikzlibrary{arrows,decorations.pathmorphing,decorations.pathreplacing,backgrounds,positioning,fit}
\usepackage[conditional,light,first,bottomafter]{draftcopy}
\usepackage{graphics}
 \usepackage{graphicx}
 \usepackage[tight,footnotesize]{subfigure}
 
 
%The following packages are for the purposes of Listings to write code verbatim
% and be able to reference it as a figure, called Listing # 
%this is called as follows:
%   \begin{lstlisting}[label=verb1,caption=First verbatim,frame=tb]
%        Foo stuff to be displayed verbatim

%         Something like \LaTeXe
%    \end{lstlisting}

%       REFER TO IT AS: In~\ref{verb1} we see some verbatim code

\usepackage[T1]{fontenc}  
\usepackage[scaled=0.88]{beramono}    
\usepackage{listings}
\lstset{basicstyle=\small\ttfamily}
 
 
 

\usepackage{enumerate}

 
\usepackage{epsfig}
\usepackage{color}
\usepackage{amssymb,amsthm}
\usepackage{amsmath}
\usepackage{color}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{remark}{Remark}
\newtheorem{claim}{Claim}
\newtheorem*{acknowledgments}{Acknowledgments}
\newtheorem{definition}{Definition}
\newtheorem{exercise}{Exercise}[section]
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage[left=3.4cm,right=3.4cm,top=3cm,bottom=3cm]{geometry}
\usepackage{titling}
\newcommand{\subtitle}[1]{%
  \posttitle{%
    \par\end{center}
    \begin{center}\large#1\end{center}
    \vskip0.5em}%
}

\title{SVD and QR method}
\subtitle{Math 573 Fall 2021}
\author{Cesar Ramirez Iba\~{n}ez  }
\date{October 2021}

\begin{document}
\maketitle
  


\section{QR algorithm for eigenvalues, Big Picture First}
Ok, so now to the big picture of what the QR method for eigenvalues does: We want to find eigenvalues through successive QR factorizations or a ``QR sequence" of factorizations: it builds a sequence $\{A_i\}$ of matrices such that $A_i$ as $i\rightarrow \infty$ converges to a matrix with easy eigenvalues to find:
\begin{quote}
It converges to a matrix that is block upper triangular (when the matrix has complex conjugate pair eigenvalues, for example) and from which one can easily determine the eigenvalues. 
\end{quote}
In particular, for a special case, the notes prove the following result, though note the restrictive hypotheses required:
\begin{theorem}
Suppose that $A$ has eigenvalues $\lambda_1,...,\lambda_n$ satisfying (strict inequalities here)
\begin{align*}
|\lambda_1|>|\lambda_2|> \cdots > |\lambda_n|.
\end{align*}
Let $X$ denote the matrix whose $i$th column is an eigenvector of $A$ corresponding to $\lambda_i$ and suppose that $X^{-1}$ has an LU decomposition. Then the subdiagonal elements of the matrices $A_s$ of the basic QR algorithm tend to zero, and as for the diagonal elements:
\begin{align*}
(A_s)_{kk}\rightarrow \lambda_k\;\;\;\text{as } s\rightarrow \infty.
\end{align*}
\end{theorem}

\textbf{Remember, the previous result is restrictive in its hypotheses, but it is relatively easy to understand how it works, that's why it is given in the notes. But in general, instead of converging to an upper triangular matrix, one converges to a block upper triangular, or quasi upper triangular matrix.} \\

Let us mention first that in general QR type methods basically use the theoretically important Householder reflections to get the QR factor matrices. We first apply the reflections to get the upper Hessenberg form of a matrix $A$. Then to this upper Hessenberg form, we now use Householder reflections to successively obtain QR factorizations of a sequence of matrices which depend on previous QR factorizations. The general outline is: 












\subsection*{QR algorithm for eigenvalues (BIG PICTURE)}
\begin{enumerate}[(i)]
\item Reduce $A$ to upper Hessenberg form. Rename this Hessenberg matrix as $A$ and define $A_0:=A$. On this upper Hessenberg matrix $A_0$, apply the next step.  
\item  Apply LOOP part of successive QR factorizations to obtain a sequence of matrices $\{A_i\}$ where $A_i=R_{i-1}Q_{i-1}$ and $Q_{i-1}$ and $R_{i-1}$ are QR decompositions of the previous matrix $A_{i-1}=Q_{i-1}R_{i-1}$. This LOOP part looks like\\


\begin{lstlisting}[label=listing1, caption=LOOP ,  frame=tb] 
A_0:=A

WHILE A_i not block upper triangular enough (according to tolerance) DO
  GET Q_i, R_i such that A_i=Q_i R_i
  DEFINE A_{i+1}:=R_i Q_i
END
\end{lstlisting}

 

 
\item After some tolerance criterion, stop/halt when you got $i$ large enough such that $A_i$ is sufficiently block upper triangular-looking.
\item From this last matrix $A_i$ (for $i$ sufficiently large), read off its eigenvalues by looking at the submatrices composing the blocks on the diagonal. These are the approximations to the eigenvalues of $A$.
\end{enumerate}
We illustrate further details of steps (i) and (ii) in the section below. You can do Householder reflections however you want, Professor Wujun gave you a good and quick method to do these Householder reflections by quickly obtaining who is the vector $u$ in a Householder reflection of type
\begin{align*}
P=I-2uu^T
\end{align*}
both for the upper Hessenberg form and the QR factorization. 

\section{QR method for eigenvalues and Householder reflections}

 
\subsection{Upper Hessenberg form of a matrix}
As mentioned, first reduce $A$ to its upper Hessenberg form, where we have:
\begin{theorem}
Let $A$ be an $n\times n$ real matrix. Then there exists an orthogonal matrix $Q$ such that $Q^TAQ$ is quasi-triangular (i.e., $A$ is block triangular with each diagonal block of order $\leq 2$. Moreover, $Q$ may be chosen so that any $2\times 2$ diagonal block of $Q^TAQ$ has only complex eigenvalues (which must be complex conjugate since a polynomial with real coefficients can only have complex roots in conjugate pairs). Another terminology used is to call $A$ upper Hessenberg if $a_{ij}=0$ when $i>j+1$. So if $A$ is real, we are saying there exists an orthogonal matrix $Q$ making $Q^TAQ$ upper Hessenberg.
\end{theorem}


We do have though, the following result for the special case when $A$ is real and symmetric, since $Q^TAQ$ would also have to be symmetric, so we must have that it being upper triangular would imply the following: 
\begin{corollary}
If $A$ is real and symmetric, there exists an orthogonal matrix $Q$ such that $Q^TAQ$ is tridiagonal.
\end{corollary}

\subsection{Step (i) of algorithm: Householder reflections for upper Hessenberg form}

We start off with an $n\times n$ matrix $A$. The algorithm used applies successive Householder reflecions which can be viewed as successive matrix operators $P_k$ applied on both sides of the matrix $A$ which reduce/transform the matrix $A$, one column at a time (according to certain information from each row and column) until we reach the upper Hessenberg matrix $H$ similar to $A$. To get a block upper triangular matrix we need $n-2$ column modifications. This is summarized neatly with matrix transformation notation as
\begin{align}\label{get_Hessenberg_successive}
H=P_{n-2}P_{n-1}\cdots P_{1}A P_{1}P_2\cdots P_{n-2}
\end{align} 
The notes go into more detail about how to define and motivate the Householder reflections $P_r$ in \eqref{house_reflectors_Hessenberg}, but in short we define matrices $P_r$ through column vectors $\mathbf{u}_r$ that are zero in the first $r$ components (to maintain certain properties untouched) and rest of components carefully chosen in order to define the Householder reflections $P_r$ by
\begin{align}\label{house_reflectors_Hessenberg}
P_r:=  I- \mathbf{u}_r \mathbf{u}_r^T/(2K_r^2)
\end{align}
where $2K_r^2=S_r^2+a_{r+1,r}^{(r-1)} S_r$ and $S_r$ is defined below in \eqref{defn_S_r_Hessenberg}.
Starting with $A_0:=A$, we define $A_r=P_r^T A_{r-1} P_r$ for $r=1,2,...,n-2$. We define the $\mathbf{u}_r$ componentwise, by first letting $a_{ij}^{(r-1)}$ denote the $ij$th element of $A_{r-1}$ and setting
\begin{align}\label{defn_S_r_Hessenberg}
S_r=\text{sgn}\left(a_{r+1,r}^{(r-1)} \right)\left(\sum_{i=r+1}^n [a_{ir}^{(r-1)}]^2\right)^{1/2},
\end{align}
we define $\mathbf{u}_r= ((\mathbf{u}_r)_1, (\mathbf{u}_r)_2,...,(\mathbf{u}_r)_n)^T$ componentwise by
\begin{align*}
&(\mathbf{u}_r)_i=0, \;\; i=1,...,r,\\
&(\mathbf{u}_r)_{r+1}=a_{r+1,r}^{(r-1)}+S_r, \\
&(\mathbf{u}_r)_i=a_{ir}^{(r-1)}, \;\; i=r+2,...,n.
\end{align*}
With the previous, we define $A_r=P_r^T A_{r-1} P_r$ for $r=1,2,...,n-2$ starting from $A_0=A$, like said previously, which all in all is summarized as obtaining the Hessenberg matrix $H$ similar to $A$ as in \eqref{get_Hessenberg_successive}.\\

Notice since similar matrices have the same eigenvalues, the $H$ Hessenberg matrix similar to $A$ has the same eigenvalues as $A$. If through Householder reflections we reach a Hessenberg matrix, for example, with a lone diagonal element, then that diagonal element would be an eigenvalue. this is because of the property that if $H$ is an $n\times n$ matrix made of block matrices, for instance as in
\begin{align*}
H=\begin{pmatrix}
D_1 & C \\ 
0 & D_2
\end{pmatrix}
\end{align*} with $D_1$ an $m\times m$ block matrix and $D_2$ an $l\times l$ block matrix, then
\begin{align*}
\det (H-\lambda I_n) =
\det \begin{pmatrix}
D_1-\lambda I_m & C \\ 
0 & D_2-\lambda I_l
\end{pmatrix}=
\det (D_1-\lambda I_m)\cdot\det (D_2-\lambda I_l).
\end{align*}

 For instance, if we had obtained through Householder reflections the Hessenberg form
\begin{align*}
P_{n-2}P_{n-1}\cdots P_{1}A P_{1}P_2\cdots P_{n-2}=H=
\begin{pmatrix}
2 & -1/\sqrt{2} & 5/\sqrt{2} \\ 
-\sqrt{2} & 2 & 2 \\ 
0 & 0 & -2
\end{pmatrix} 
\end{align*}
then we know one of the eigenvalues of $A$ is $-2$. 





\section{Singular Value Decomposition Project}

The \textbf{singular value decomposition} of an $m\times n$ matrix $M$ is a factorization of the form 
$$
M = U\Sigma V^{T}
$$
where $U$ is an $m\times m$ orthogonal matrix, $\Sigma$ is an $m\times n$ rectangular diagonal matrix with non-negative real numbers on the diagonal, and V is an $n\times n$ orthogonal matrix. 

\subsection{How many types of SVDs?}
There can be more than one type of SVD \textbf{for a rectangular} $m\times n$ matrix $M$ (i.e. when $m\neq n$) as can be seen in the literature since we have at least:
\begin{enumerate}
\item \textbf{SVD on a padded matrix:} If $M\in\mathbb{F}^{m\times n}$ is augmented through additional zero (columns or rows) to a matrix $B\in \mathbb{F}^{N\times N}$ with $N:=\max\{m,n\}$, and you do ``square" SVD. You end up factoring $B=U\Sigma V^T$ but you get according to the case present:
\begin{itemize}
\item If $n<m$
\begin{align*}
B= [M\; 0]=U \underbrace{
\begin{bmatrix}
\Sigma_1 & 0 \\ 
0 & 0
\end{bmatrix}}_{{\Sigma}}
\underbrace{\begin{bmatrix}
V_1 & 0 \\ 
0 & V_2
\end{bmatrix}^T}_{V^T}
\end{align*}
The matrix $V_1$ is of size $n\times n$, $V_2$ of size $(m-n)\times (m-n)$. $\Sigma_1$ a matrix of size $n\times n$. $U$ is of size $m\times m$.
\item If $n>m$ 
\begin{align*}
B= 
\begin{bmatrix}
M \\ 
0
\end{bmatrix} 
 =
\underbrace{\begin{bmatrix}
U_1 & 0 \\ 
0 & U_2
\end{bmatrix}^T}_{U} 
  \underbrace{
\begin{bmatrix}
\Sigma_1 & 0 \\ 
0 & 0
\end{bmatrix}}_{{\Sigma}}
V^T
\end{align*}
$\Sigma_1$ a matrix of size $m\times m$, $U_1$ an $m\times m$ matrix and $U_2$ a size $(n-m)\times (n-m)$, while $V$ of size $n\times n$.
\end{itemize}
In general for the padded case we have $\Sigma_1$ a matrix of size $\min\{m,n\}\times \min\{m,n\}$. $U, V, \Sigma$ are of size $\max\{m,n\}\times \max\{m,n\}$. In this case it is true that $\Sigma\Sigma^T=\Sigma^T\Sigma$.

\item \textbf{Complete SVD no-padding:} You can go straightforward to do SVD on 
\begin{align*}
M=U
\begin{bmatrix}
\Sigma_1 \\ 
0
\end{bmatrix} V_1^T,\;\;\;\text{ when }n<m 
\end{align*}
or 
\begin{align*}
M=U_1
\begin{bmatrix}
\Sigma_1 & 
0
\end{bmatrix} V^T,\;\;\;\text{ when }m<n 
\end{align*}
In this case it is NOT true that $\Sigma\Sigma^T=\Sigma^T\Sigma$.  
\item \textbf{Economy SVD:} Alternatively, the matrix $M$ can be represented as the sum of rank-one matrices,
$$
M =U\Sigma V^T= \sum_{i=1}^{\min{(n,m)}} \sigma_i u_i v_i^{T}
$$
where $u_i$ and $v_i$ are the i-th columns of the matrix $U$ and $V$ and $\sigma_i$ the i-th diagonal element of $\Sigma$. Here if $p:=\min\{m,n\}$ then $U\in\mathbb{F}^{m\times p}$ has orthonormal columns containing the left-singular vectors, $\Sigma\in\mathbb{F}^{p\times p}$ is a diagonal matrix containing singular values in non-increasing order and $V\in\mathbb{F}^{n\times p}$ has orthonormal columns made of the right-singular vectors.
In this case it is true that $\Sigma\Sigma^T=\Sigma^T\Sigma$.

\item \textbf{Compact SVD:} If $k:=\text{rank}(M)<\min\{m,n\}$, then the compact SVD form allows matrix $M$ to be represented as the sum of even less rank-one matrices:
\begin{equation}\label{rank_k}
M =U\Sigma V^T= \sum_{i=1}^{k} \sigma_i u_i v_i^{T}
\end{equation}
where $u_i$ and $v_i$ are the i-th columns of the matrix $U$ and $V$ and $\sigma_i$ the i-th diagonal element of $\Sigma$. Here $U\in\mathbb{F}^{m\times k}$ has orthonormal columns containing the left-singular vectors, $\Sigma\in\mathbb{F}^{k\times k}$ is a diagonal matrix containing singular values in non-increasing order and $V\in\mathbb{F}^{n\times k}$ has orthonormal columns made of the right-singular vectors. In this case it is true that $\Sigma\Sigma^T=\Sigma^T\Sigma$.
\end{enumerate}
\subsection{Truncated SVD}
We can approximate $M$ by even less rank-one operators than \eqref{rank_k}. The idea is to throw away the smaller singular values and chop off to get
\begin{align*}
M\approx \sum_{i=1}^{n_{\text{small}}}\sigma_i u_i v_i^{T}
\end{align*}

\subsection{The project itself}
Alternatively, the matrix $M$ can be represented as the sum of rank-one matrices,
$$
M = \sum_{i=1}^{\min{(n,m)}} \sigma_i u_i v_i^{T}
$$
where $u_i$ and $v_i$ are the i-th columns of the matrix $U$ and $V$ and $\sigma_i$ the i-th diagonal element of $\Sigma$. In this project, we assume all the diagonal entries $\sigma_i$ are non-negative.


The singular value decomposition (SVD) generalizes the eigen-decomposition of a square normal matrix with an orthonormal eigen-basis to any $ m\times n $ matrix. 

The goal of this project is to develop a numerical algorithm to perform the singular value decomposition, based on QR decomposition taught in lecture, and the QR algorithm taught in class. We will also apply the singular value decomposition to low-rank matrix approximation.

\section*{(a)}  
Show that the columns of $V$ (right-singular vectors) are eigenvectors of $M^T M$ and 
Show that the columns of $U$ (left-singular vectors) are eigenvectors of $M M^T$, i.e. 
$$
M^T M = V \Sigma^T \Sigma V^T
\quad
\mbox{and}
\quad 
M M^T = U \Sigma \Sigma^T U^T.
$$ 

\section*{(b)}

Write a python function to perform the singular value decomposition, based on the QR algorithm applied to $M M^T$. You may use Hessenberg from (la.hessenberg) and QR decomposition in the scipy.linalg module. Type help(la.hessenberg) and help(la.qr) for more details. The function should returns two matrices $U$ and $\Sigma V^T$.
(No credit if you use the la.svd directly!) 

\section*{(c)}
Derive a formula to find the diagonal matrix $\Sigma$ from the product $\Sigma V^T$. Show all steps!

\section*{(d)}
We say $\tilde{M}$ is the best rank $r$ ($r<\min(m,n)$) approximation of matrix $M$ if 
$$
\tilde{M} = M = U \tilde{\Sigma} V^{T}
$$
where $\tilde{\Sigma}$ is the same matrix as $\Sigma$ except that it contains only the $r$ largest singular values (the other singular values are replaced by zero).
Load the file image.npy as the matrix $M$, which is the pixel matrix of a hand written zero.
Find its best rank $10$ approximation $\tilde{M}$ using the function in part b) and plot its corresponding image. 
Calculate the Frobenius norm of $$M - \tilde{M}$$ by LA.norm().
The Frobenius norm of matrix $A$ is defined as
$$
\| A \|_F = \sqrt{\sum_{i,j=1}^{i=n,j=m} a_{ij}^2}.
$$



\subsection{Getting $\Sigma V^T$ and $U$}

Note that you are getting $\Sigma\Sigma^T$ as the diagonal matrix and you get $U$ from the orthogonal transformations such that $MM^T=U\Sigma\Sigma^TU^T$ but you also assume that $M=U\Sigma V^T$ so that $U^T M=\Sigma V^T$. Since you know $U$ and $M$, this last relation gives you $\Sigma V^T$.

\subsection{What about $\Sigma$?}
I would define $\Sigma$ through the square roots of $\Sigma\Sigma^T$'s eigenvalues and be done with it. For some reason, everytime I get $\Sigma\Sigma^T$, I get the eigenvalues in the correct order. 

\subsection{What about $V$?}
The problem is that applying the QR algorithm to $M^TM$ seems to give $V$ but I don't think its eigenvectors align with those for $MM^T$ in order 


ANOTHER OPTION: Now how about now that you know who $\Sigma$ is, dividing the columns of $\Sigma V^T$ by the adequate singular values $\sigma_i$ to get a potential $V^T$? The tricky part here becomes that to zero singular values you must associate an eigenvector. I would associate a dimension to some kernel somehow and find an orthonormal basis for it. 

But actually you don't have to worry about that because in a compact SVD you don't need those zero right singular vectors (i.e., vectors corresponding to the zero singular value), since you would throw them out. So for a compact SVD you can eliminate those vectors from the $V$ matrix as I show below.


\section{What if we converged to something good enough?}

For the project we have the matrix $A=MM^T$, or $A=M^TM$, whichever one you want to start with (most likely started with $MM^T$. Note that $MM^T$ is a symmetric matrix, with real coefficients. Also, it (and likewise $M^TM$) is positive definite since
\begin{align*}
x^T(MM^T)x= (M^Tx)^T (M^T x)=\|M^Tx\|^2_2\geq 0
\end{align*}
 Therefore, supposing the eigenvalues
\begin{align*}
\lambda_1>\lambda_2> \cdots >\lambda_n
\end{align*}
we converge along the diagonal to the eigenvalues, from the most basic convergence result. \textbf{But actually the symmetry of the matrix allows this to converge in particular to a diagonal matrix} $A_i\rightarrow \text{diag}[\lambda_1,...,\lambda_n]$. Note from the QR algorithm you have:
\begin{align*}
A_i=Q_i R_i\hspace{0.1in}\rightarrow
A_{i+1}=R_{i}Q_i
\end{align*}
so that supposing at step $k$ you are within tolerance level to a diagonal matrix, so $A_k\approx D$ a diagonal matrix. Then note from the way you successively operated starting from the original Hessenberg matrix $A_0$, that 
\begin{align*}
D\approx A_k=R_{k-1} Q_{k-1}&=(Q_{k-1})^T \underbrace{Q_{k-1} R_{k-1}}_{=A_{k-1}} Q_{k-1}\\
 &= (Q_{k-1})^T R_{k-2}Q_{k-2}Q_{k-1}\\
 &=(Q_{k-1})^T(Q_{k-2})^T Q_{k-2}R_{k-2}Q_{k-2}Q_{k-1}\\
 &=(Q_{k-1})^T(Q_{k-2})^T A_{k-2}Q_{k-2}Q_{k-1}\\
 &\vdots  \\
 &= (Q_{k-1})^T(Q_{k-2})^T\cdots (Q_{0})^T A_{0} Q_0\cdots Q_{k-2}Q_{k-1}
\end{align*}
so that if $H_k:= Q_0\cdots Q_{k-2}Q_{k-1}$ (an orthogonal matrix, because it is a product of orthogonal matrices) we have that
\begin{equation}\label{diagonal_and_qr}
D\approx A_k= H_k^T A_0 H_k
\end{equation}
 So that $H_k$ approximately diagonalizes $A_0$ which is the Hessenberg form of the original matrix $A$. Thus we could interpret $H_k$ as being an approximation to the necessary change of basis matrix that diagonalizes $A$, with the columns of $H_k$ containing approximations to the eigenvectors. Note that the matrix $A=M^TM$ is positive semidefinite since $x^TAx=x^TM^TMx=(Mx)^TMx=\|Mx\|^2\geq 0$ so it has non-negative eigenvalues, and taking the nonnegative roots of the eigenvalues $$+\sqrt{\lambda_i}=:\sigma_i$$ then\footnote{If we had a square matrix to begin with, then $\Sigma$ would not be ``rectangular" diagonal, but rather, perfectly diagonal and it would be true that $D=\Sigma \Sigma^T=\Sigma^T \Sigma$. But since we are in the case $m\neq n$ in general, we cannot affirm $\Sigma=\Sigma^T$ and $\Sigma \Sigma^T=\Sigma^T \Sigma$. }
 \begin{align*}
D=\Sigma \Sigma^T
 \end{align*}
Note that $A_0$ in \eqref{diagonal_and_qr} was the Hessenberg form of the original matrix $A=MM^T$ so it was obtained by applying successive orthogonal transformations such that $A_0=P^TAP$ is in upper Hessenberg form, therefore combining with equation \eqref{diagonal_and_qr} we have
\begin{align}\label{defining_U}
D=\Sigma \Sigma^T\approx (PH_k)^T MM^T (PH_k)=U^T MM^T U
\end{align}
with $U:=PH_k$ an orthogonal matrix. The previous implies
\begin{align*}
U\Sigma \Sigma^T U^T=MM^T
\end{align*} 


\section{Potential Difficulties project}
In short the biggest potential difficulty is that you get something like
\begin{align*}
\Sigma\Sigma^T=\begin{bmatrix}
\mathbf{0} & • & • \\ 
• & \Sigma_1^2 & • \\ 
• &  & \mathbf{0}
\end{bmatrix} 
\end{align*}
where
\begin{equation}
\Sigma_1^2=\text{diag}[\sigma_1^2, \sigma_2^2,...,\sigma_r^2]\in\mathbb{R}^{r\times r}
\end{equation}
with $\sigma_i^2>0$ and $r=$rank$(M)$.



Imagine that the following happens: You get an SVD of form
\begin{align*} U\Sigma V^T=
\underbrace{\begin{bmatrix}
u_{11} & u_{12} & u_{13} \\ 
u_{21} & u_{22} & u_{23} \\ 
u_{31} & u_{32} & u_{33}
\end{bmatrix} }_{\in\mathbb{F}^{3\times 3}}
\underbrace{\begin{bmatrix}
0 & 0 & 0 & 0 \\ 
0 & \sigma_2 & 0 & 0 \\ 
0 & 0 & \sigma_3 & 0
\end{bmatrix}}_{\in\mathbb{F}^{3\times 4}}
\underbrace{\begin{bmatrix}
v_{11} & v_{12} & v_{13} & v_{14}\\ 
v_{21} & v_{22} & v_{23} & v_{24}\\ 
v_{31} & v_{32} & v_{33} & v_{34}\\
v_{41} & v_{42} & v_{43} & v_{44}
\end{bmatrix}}_{\in\mathbb{F}^{4\times 4}}
\end{align*}
which if we only have $U$ and $\Sigma V^T$ as in the project, becomes
\begin{align}\label{completeSVD}
 M=U(\Sigma V^T)=
\underbrace{\begin{bmatrix}
u_{11} & u_{12} & u_{13} \\ 
u_{21} & u_{22} & u_{23} \\ 
u_{31} & u_{32} & u_{33}
\end{bmatrix}}_{U\in \mathbb{F}^{3\times 3}}
\underbrace{
\begin{bmatrix}
0      & 0      & 0      & 0\\ 
\sigma_2 v_{12} & \sigma_2 v_{22} & \sigma_2v_{32} & \sigma_2v_{42}\\
\sigma_3 v_{13} & \sigma_3 v_{23} & \sigma_3 v_{33} & \sigma_3v_{43}
\end{bmatrix}}_{=\Sigma V^T\in \mathbb{F}^{3\times 4}}
\end{align}
Let's see what is really happening here, to understand better the rank-1 economy SVD forms:
\begin{equation}\label{em}
M=U(\Sigma V^T)=
\end{equation}
\begin{align*} 
\begin{bmatrix}
\sigma_2 u_{12}v_{12}+\sigma_3 u_{13}v_{13} & \sigma_2 u_{12}v_{22}+\sigma_3 u_{13}v_{23} & \sigma_2 u_{12}v_{32}+\sigma_3 u_{13}v_{33} & \sigma_2 u_{12}v_{42}+\sigma_3 u_{13}v_{43} \\ 
\sigma_2 u_{22}v_{12}+\sigma_3 u_{23}v_{13} & \sigma_2 u_{22}v_{22}+\sigma_3 u_{23}v_{23} & \sigma_2 u_{22}v_{32}+\sigma_3 u_{23}v_{33} & \sigma_2 u_{22}v_{42}+\sigma_3 u_{23}v_{43} \\ 
\sigma_2 u_{32}v_{12}+\sigma_3 u_{33}v_{13} & \sigma_2 u_{32}v_{22}+\sigma_3 u_{33}v_{23} & \sigma_2 u_{32}v_{32}+\sigma_3 u_{33}v_{33} & \sigma_2 u_{32}v_{42}+\sigma_3 u_{33}v_{43}
\end{bmatrix} 
\end{align*}
\begin{align*}
=\sigma_2 
\begin{bmatrix}
 u_{12}v_{12} & u_{12}v_{22} &  u_{12}v_{32} &  u_{12}v_{42} \\ 
u_{22}v_{12} &  u_{22}v_{22} &  u_{22}v_{32} &  u_{22}v_{42} \\ 
u_{32}v_{12} &  u_{32}v_{22} &  u_{32}v_{32} &  u_{32}v_{42}
\end{bmatrix} + \sigma_3
\begin{bmatrix}
 u_{13}v_{13} & u_{13}v_{23} &  u_{13}v_{33} &  u_{13}v_{43} \\ 
u_{23}v_{13}  &  u_{23}v_{23} &  u_{23}v_{33} &  u_{23}v_{43} \\ 
u_{33}v_{13}  &  u_{33}v_{23} &  u_{33}v_{33} &  u_{33}v_{43}
\end{bmatrix}
\end{align*}

\begin{align*}
=\sigma_2 \begin{bmatrix}
u_{12} \\ 
u_{22} \\ 
u_{32}
\end{bmatrix} 
\begin{bmatrix}
v_{12} & v_{22} & v_{32} & v_{42}
\end{bmatrix} +
\sigma_3 \begin{bmatrix}
u_{13} \\ 
u_{23} \\ 
u_{33}
\end{bmatrix} 
\begin{bmatrix}
v_{13} & v_{23} & v_{33} & v_{43}
\end{bmatrix} 
\end{align*}
\begin{equation}\label{economy_example}
=\sigma_2 \mathbf{u}_{2}\mathbf{v}_{2}^T+\sigma_3 \mathbf{u}_{3}\mathbf{v}_{3}^T\hspace{1in}\text{(Compact SVD)}
\end{equation}
Note how the compact SVD reflects the rank of the original matrix, which was 2. If someone asked you to obtain the best rank 1 approximation to $M$, and $\sigma_2>\sigma_3$, then the best rank one approximation is from retaining only the largest summand in \eqref{economy_example}, which since $\mathbf{u}_i$ and  $\mathbf{v}_i$ are orthonormal, or in particular, have norm one, then the largest summand is
\begin{align*}
\sigma_2 
\begin{bmatrix}
 u_{12}v_{12} & u_{12}v_{22} &  u_{12}v_{32} &  u_{12}v_{42} \\ 
u_{22}v_{12} &  u_{22}v_{22} &  u_{22}v_{32} &  u_{22}v_{42} \\ 
u_{32}v_{12} &  u_{32}v_{22} &  u_{32}v_{32} &  u_{32}v_{42}
\end{bmatrix} 
\end{align*}
\begin{equation}\label{truncated_example}
\hspace{1.2in}=\sigma_2 \mathbf{u}_{2}\mathbf{v}_{2}^T\approx M \hspace{1in}\text{(Truncated SVD)}
\end{equation}

Thus we are approximating $M$ in \eqref{em} by \eqref{truncated_example}
But note we could have done the factorization in \eqref{completeSVD} a bit different: We could have realized we drop the zero row of $\Sigma V^T$ (the first one) and the corresponding column vector of $U$ (also the first one) to factor $M$ as 

\begin{align}\label{completeSVD2}
 M=\widetilde{U}\widetilde{\Sigma}_{V^T}=
\underbrace{\begin{bmatrix}
 u_{12} & u_{13} \\ 
  u_{22} & u_{23} \\ 
 u_{32} & u_{33}
\end{bmatrix}}_{ \in \mathbb{F}^{3\times 2}}
\underbrace{
\begin{bmatrix}
\sigma_2 v_{12} & \sigma_2 v_{22} & \sigma_2v_{32} & \sigma_2v_{42}\\
\sigma_3 v_{13} & \sigma_3 v_{23} & \sigma_3 v_{33} & \sigma_3v_{43}
\end{bmatrix}}_{ \in \mathbb{F}^{2\times 4}}
\end{align}
where $\widetilde{\Sigma}_{V^T}$ replaces $\Sigma V^T$ by eliminating the necessary row. 




\newpage
\newpage







\end{document}