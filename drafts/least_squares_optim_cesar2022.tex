
  
  \documentclass[12pt]{article}
 \usepackage{tikz-cd}
\usepackage{tikz}
\usetikzlibrary{arrows,decorations.pathmorphing,decorations.pathreplacing,backgrounds,positioning,fit}
\usepackage[conditional,light,first,bottomafter]{draftcopy}
\usepackage{graphics}
 \usepackage{graphicx}
 \usepackage[tight,footnotesize]{subfigure}
 \usepackage{scalerel}
 \usepackage{cancel}
 
 

\usepackage{enumerate}

 
\usepackage{epsfig}
\usepackage{color}
\usepackage{amssymb,amsthm}
\usepackage{amsmath}
\usepackage{color}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{remark}{Remark}
\newtheorem*{acknowledgments}{Acknowledgments}
\newtheorem{definition}{Definition}
\newtheorem{exercise}{Exercise}[section]
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage[left=3.2cm,right=3.2cm,top=3.8cm,bottom=4cm]{geometry}


 

 
\title{Optimization, orthogonality and linear systems}
 

\author{C\'{e}sar Ram\'{i}rez Ib\'{a}\~{n}ez  }
\date{October 5, 2022 }
\begin{document}


\section{Optimization view of linear systems}
Solving a linear system of equations is a special case of linear regression, which at the end of the day is an optimization problem. Solving a system of equations $$Ax=b$$ can be seen as achieving the minimum value
\begin{equation}\label{optim0}
\min_{x\in\mathbb{R}^n} \|Ax-b\|^2 \hspace{0.2in}\big(=0 \text{ if an exact solution exists} \big)
\end{equation}
As mentioned previously, this is \textbf{classical least-squares regression}, where the cost function is the square of the Euclidean norm $\|\cdot\|^2$. 

If the system of equations has no solutions (is inconsistent), the optimization problem \eqref{optim0} returns the next best thing, which is equivalent to the smallest value of \eqref{optim0} possible. It will be the case that

\begin{equation}\label{optim1}
\min_{x\in\mathbb{R}^n} J(x)=
\min_{x\in\mathbb{R}^n} \|Ax-b\|^2 >0 \leftarrow\text{ if no solution exists}
\end{equation}
A value of $x$ that achieves the minimum exists? Is it unique?   


\subsection{Geometric Solution the previous?}
Of course, we can use calculus to solve \eqref{optim1}, i.e., find the unique optimal value. But we can try the following geometric argument:
\begin{enumerate}
\item As you can see in 2-D (maybe in 3-D too), to get from a point $b\in\mathbb{R}^n$ to a hyperplane (in 2-D this is a straight line), you draw a line $\mathcal{L}$ from $b$ to the hyperplane, such that $\mathcal{L}$\textbf{ is orthogonal to the hyperplane}. 

\end{enumerate} 












\section{Some assignment hints}



In problem 4 we have the matrix
\begin{align*}
A=
\begin{bmatrix}
1 & 1 & 3 \\ 
2 & 0 & 2 \\ 
3 & 1 & 5 \\ 
0 & 1 & 2
\end{bmatrix} 
\end{align*}

When does $A^TA$ have an inverse? Let us determine the rank of the column space of $A$, which for the moment let us denote $\mathbf{C}(A)$. We check if the columns are linearly independent because

\begin{equation}
\textbf{rank}(A^TA)=\textbf{rank}(AA^T)=\textbf{rank}(A)
\end{equation}
\begin{align*}
A\mapsto \left[\begin{array}{rrr}
1 & 1 & 3 \\ 
0 & -2 & -4 \\ 
0 & -2 & -4\\ 
0 & 1 & 2
\end{array}\right]\mapsto
 \left[\begin{array}{rrr}
1 & 1 & 3 \\ 
0 & 1 & 2 \\ 
0 & 0 & 0\\ 
0 & 0 & 0
\end{array}\right]
\end{align*}
so the column space has rank 2! Thus we cannot guarantee the inverse $A^TA$ exists. 


\end{document}